import streamlit as st
import re
from collections import Counter
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="AI vs Human æ–‡ç« åµæ¸¬å™¨",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# è‡ªè¨‚ CSS æ¨£å¼
st.markdown("""
    <style>
    .main {
        max-width: 1000px;
        margin: 0 auto;
    }
    .header-title {
        background: linear-gradient(135deg, #218D8D 0%, #134252 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 2.5em;
        font-weight: 700;
        margin-bottom: 10px;
    }
    .metric-card {
        background: #fcfcf9;
        padding: 20px;
        border-radius: 8px;
        border: 1px solid #5e5240;
    }
    .verdict-ai {
        background-color: rgba(192, 21, 47, 0.1);
        border: 1px solid #c01547;
        color: #c01547;
    }
    .verdict-human {
        background-color: rgba(34, 197, 94, 0.1);
        border: 1px solid #22c55e;
        color: #22c55e;
    }
    .verdict-uncertain {
        background-color: rgba(168, 75, 47, 0.1);
        border: 1px solid #a84b2f;
        color: #a84b2f;
    }
    </style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ– session state
if 'analysis_result' not in st.session_state:
    st.session_state.analysis_result = None
if 'analysis_history' not in st.session_state:
    st.session_state.analysis_history = []

# ==================== ç‰¹å¾µæå–å‡½æ•¸ ====================

def extract_features(text):
    """æå–æ–‡æœ¬çš„å¤šç¶­ç‰¹å¾µ"""
    
    # é è™•ç†
    words = re.findall(r'[\w\u4e00-\u9fa5]+', text.lower())
    sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text) if s.strip()]
    chars = len(text)
    
    # 1. å¥é•·å·®ç•° (Burstiness)
    if sentences:
        sentence_lengths = [len(re.findall(r'[\s\u4e00-\u9fa5]+', s)) for s in sentences]
        avg_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0
        variance = sum((l - avg_length) ** 2 for l in sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0
        burstiness = (variance ** 0.5) / avg_length if avg_length > 0 else 0
    else:
        burstiness = 0
    
    # 2. è©å½™å¤šæ¨£æ€§ (TTR - Type-Token Ratio)
    unique_words = len(set(words))
    ttr = unique_words / len(words) if words else 0
    
    # 3. ç¬¦è™Ÿä½¿ç”¨ç‡
    punctuation = re.findall(r'[ï¼ï¼Ÿï¼Œï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€ã€‘â€¦]', text)
    punctuation_rate = len(punctuation) / len(text) if text else 0
    
    # 4. å¹³å‡è©é•·
    avg_word_length = sum(len(w) for w in words) / len(words) if words else 0
    
    # 5. é‡è¤‡è©ç‡
    word_freq = Counter(words)
    repeated_words = sum(1 for f in word_freq.values() if f > 2)
    top_repeated = repeated_words / unique_words if unique_words > 0 else 0
    
    # 6. é€£æ¥è©ä½¿ç”¨ç‡
    connectors = ['è€Œä¸”', 'å› ç‚º', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'é›–ç„¶', 'ç‚ºäº†', 'ç”±æ–¼', 'åŸºæ–¼', 'é‘’æ–¼']
    connector_count = sum(1 for c in connectors if c in text)
    connector_rate = connector_count / len(sentences) if sentences else 0
    
    # 7. å¥å­è¤‡é›œåº¦
    comma_count = len(re.findall(r'[ï¼Œ,]', text))
    complexity = comma_count / len(sentences) if sentences else 0
    
    # 8. Perplexity ç°¡åŒ–ä¼°è¨ˆï¼ˆåŸºæ–¼è©å½™åˆ†ä½ˆï¼‰
    total_words = len(words)
    unique_ratio = unique_words / total_words if total_words > 0 else 0
    perplexity_score = 1 - unique_ratio  # AI æ–‡æœ¬å¾€å¾€é‡è¤‡åº¦é«˜
    
    # 9. è©å½™é »ç‡åˆ†ä½ˆï¼ˆZipf's Lawï¼‰
    sorted_freqs = sorted(word_freq.values(), reverse=True)
    if len(sorted_freqs) > 10:
        top_10_ratio = sum(sorted_freqs[:10]) / sum(sorted_freqs)
    else:
        top_10_ratio = 1.0
    
    return {
        'burstiness': max(0, min(1, burstiness / 2)),
        'ttr': ttr,
        'punctuation_rate': punctuation_rate,
        'avg_word_length': max(0, min(1, avg_word_length / 8)),
        'top_repeated': top_repeated,
        'connector_rate': max(0, min(1, connector_rate)),
        'complexity': max(0, min(1, complexity / 0.5)),
        'perplexity_score': perplexity_score,
        'top_10_ratio': top_10_ratio,
        'word_count': len(words),
        'sentence_count': len(sentences),
        'char_count': chars,
        'unique_words': unique_words,
        'word_freq_dist': dict(word_freq.most_common(20))
    }

# ==================== AI åµæ¸¬æ¨¡å‹ ====================

def detect_ai(features):
    """ä½¿ç”¨åŠ æ¬Šç‰¹å¾µè¨ˆç®— AI ç”Ÿæˆæ©Ÿç‡"""
    
    weights = {
        'burstiness': 0.20,           # AI èªå¥é•·åº¦è¼ƒå‡å‹»
        'ttr': -0.15,                 # äººå·¥é€šå¸¸æ›´å¤šæ¨£
        'punctuation_rate': 0.15,     # AI ä½¿ç”¨æ¨™é»æ›´è¦å¾‹
        'avg_word_length': -0.10,     # äººå·¥è©å½™æ›´è¤‡é›œ
        'top_repeated': 0.15,         # AI é‡è¤‡ä½¿ç”¨æŸäº›è©å½™
        'connector_rate': -0.15,      # äººå·¥æ›´å¤šä½¿ç”¨é€£æ¥è©
        'complexity': -0.10,          # äººå·¥å¥å­çµæ§‹æ›´è¤‡é›œ
        'perplexity_score': 0.10,     # AI è¤‡é›œåº¦è¼ƒä½
        'top_10_ratio': 0.10          # AI è©å½™é›†ä¸­åº¦é«˜
    }
    
    ai_score = 0.5  # åˆå§‹ä¸­æ€§åˆ†æ•¸
    
    # æ ¹æ“šç‰¹å¾µèª¿æ•´åˆ†æ•¸
    ai_score += (1 - features['burstiness']) * weights['burstiness']
    ai_score += features['ttr'] * weights['ttr']
    ai_score += features['punctuation_rate'] * weights['punctuation_rate']
    ai_score += features['avg_word_length'] * weights['avg_word_length']
    ai_score += features['top_repeated'] * weights['top_repeated']
    ai_score += features['connector_rate'] * weights['connector_rate']
    ai_score += features['complexity'] * weights['complexity']
    ai_score += features['perplexity_score'] * weights['perplexity_score']
    ai_score += features['top_10_ratio'] * weights['top_10_ratio']
    
    # æ ¹æ“šæ–‡æœ¬é•·åº¦èª¿æ•´ä¿¡å¿ƒåº¦
    min_words = 50
    confidence = max(0, min(1, features['word_count'] / min_words))
    ai_score = 0.5 + (ai_score - 0.5) * confidence
    
    return max(0, min(1, ai_score))

# ==================== UI æ§‹å»º ====================

# æ¨™é¡Œ
col1, col2 = st.columns([3, 1])
with col1:
    st.markdown('<div class="header-title">ğŸ” AI vs Human æ–‡ç« åµæ¸¬å™¨</div>', unsafe_allow_html=True)
    st.markdown("ä½¿ç”¨å¤šç¶­ç‰¹å¾µåˆ†ææŠ€è¡“ï¼Œæª¢æ¸¬æ–‡æœ¬æ˜¯å¦ç”± AI ç”Ÿæˆ")

with col2:
    if st.button("ğŸ“‹ æŸ¥çœ‹æ­·å²", use_container_width=True):
        st.session_state.show_history = not st.session_state.get('show_history', False)

# è¼¸å…¥å€åŸŸ
st.markdown("---")
st.subheader("ğŸ“ è¼¸å…¥æ–‡æœ¬")

text_input = st.text_area(
    "è²¼ä¸Šä½ çš„æ–‡æœ¬å…§å®¹",
    placeholder="è¼¸å…¥è‡³å°‘ 20 å€‹å­—ä»¥ç²å¾—æº–ç¢ºçµæœ...",
    height=200,
    label_visibility="collapsed"
)

# é¡¯ç¤ºå­—æ•¸
if text_input:
    st.caption(f"å­—æ•¸ï¼š{len(text_input)} å­— | è©æ•¸ï¼š{len(re.findall(r'[\w\u4e00-\u9fa5]+', text_input))} è©")

# åˆ†ææŒ‰éˆ•
col1, col2, col3 = st.columns([1, 1, 2])
with col1:
    analyze_btn = st.button("âœ¨ é–‹å§‹åˆ†æ", use_container_width=True, type="primary")
with col2:
    clear_btn = st.button("æ¸…ç©º", use_container_width=True)

if clear_btn:
    st.session_state.analysis_result = None
    st.rerun()

# ==================== åˆ†æé‚è¼¯ ====================

if analyze_btn:
    if len(text_input.strip()) < 20:
        st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ 20 å€‹å­—çš„æ–‡æœ¬")
    else:
        with st.spinner("åˆ†æä¸­... â³"):
            # æå–ç‰¹å¾µ
            features = extract_features(text_input)
            
            # è¨ˆç®—åˆ†æ•¸
            ai_score = detect_ai(features)
            human_score = 1 - ai_score
            
            # ä¿å­˜çµæœåˆ° session state
            st.session_state.analysis_result = {
                'timestamp': datetime.now(),
                'text_preview': text_input[:100] + "..." if len(text_input) > 100 else text_input,
                'text_full': text_input,
                'ai_score': ai_score,
                'human_score': human_score,
                'features': features
            }
            
            # æ·»åŠ åˆ°æ­·å²
            st.session_state.analysis_history.append(st.session_state.analysis_result)

# ==================== çµæœé¡¯ç¤º ====================

if st.session_state.analysis_result:
    result = st.session_state.analysis_result
    ai_percent = int(result['ai_score'] * 100)
    human_percent = int(result['human_score'] * 100)
    features = result['features']
    
    st.markdown("---")
    st.subheader("ğŸ“Š åˆ†æçµæœ")
    
    # åˆ†æ•¸å¡ç‰‡
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric(
            "ğŸ¤– AI ç”Ÿæˆæ©Ÿç‡",
            f"{ai_percent}%",
            delta=None
        )
        # é€²åº¦æ¢
        st.progress(result['ai_score'], text="AI åˆ†æ•¸")
    
    with col2:
        st.metric(
            "âœï¸ äººå·¥æ’°å¯«æ©Ÿç‡",
            f"{human_percent}%",
            delta=None
        )
        # é€²åº¦æ¢
        st.progress(result['human_score'], text="Human åˆ†æ•¸")
    
    # åˆ¤æ±ºçµæœ
    st.markdown("---")
    if ai_percent > 65:
        verdict = "âš ï¸ æ¥µå¯èƒ½ç”± AI ç”Ÿæˆ"
        verdict_class = "verdict-ai"
    elif ai_percent > 50:
        verdict = "ğŸ¤” å¯èƒ½ç”± AI ç”Ÿæˆ (éœ€è¦é€²ä¸€æ­¥ç¢ºèª)"
        verdict_class = "verdict-uncertain"
    elif ai_percent > 35:
        verdict = "ğŸ“ å¯èƒ½ç”±äººå·¥æ’°å¯« (ä½†ä¹Ÿæœ‰ AI æˆåˆ†)"
        verdict_class = "verdict-uncertain"
    else:
        verdict = "âœ… é«˜åº¦å¯èƒ½ç”±äººå·¥æ’°å¯«"
        verdict_class = "verdict-human"
    
    st.markdown(
        f'<div style="padding: 20px; border-radius: 8px; text-align: center; font-size: 18px; font-weight: bold; {verdict_class}">{verdict}</div>',
        unsafe_allow_html=True
    )
    
    # è©³ç´°åˆ†ææŒ‡æ¨™
    st.markdown("---")
    st.subheader("ğŸ”¬ è©³ç´°åˆ†ææŒ‡æ¨™")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("å¥é•·å‡å‹»åº¦", f"{features['burstiness']*100:.1f}%")
    with col2:
        st.metric("è©å½™å¤šæ¨£æ€§", f"{features['ttr']*100:.1f}%")
    with col3:
        st.metric("æ¨™é»è¦å¾‹æ€§", f"{features['punctuation_rate']*100:.1f}%")
    with col4:
        st.metric("å¹³å‡è©é•·", f"{features['avg_word_length']:.2f}")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("è©å½™é‡è¤‡ç‡", f"{features['top_repeated']*100:.1f}%")
    with col2:
        st.metric("é€£æ¥è©ä½¿ç”¨ç‡", f"{features['connector_rate']*100:.1f}%")
    with col3:
        st.metric("å¥å­è¤‡é›œåº¦", f"{features['complexity']*100:.1f}%")
    with col4:
        st.metric("æ–‡æœ¬çµ±è¨ˆ", f"{features['word_count']} è©")
    
    # å¯è¦–åŒ–åˆ†æ
    st.markdown("---")
    st.subheader("ğŸ“ˆ å¯è¦–åŒ–åˆ†æ")
    
    tab1, tab2, tab3 = st.tabs(["ç‰¹å¾µé›·é”åœ–", "è©é »åˆ†ä½ˆ", "åˆ†æ•¸å°æ¯”"])
    
    with tab1:
        # é›·é”åœ–
        categories = ['å¥é•·å‡å‹»åº¦', 'è©å½™å¤šæ¨£æ€§', 'æ¨™é»è¦å¾‹æ€§', 'è©å½™é‡è¤‡ç‡', 'é€£æ¥è©ä½¿ç”¨ç‡', 'å¥å­è¤‡é›œåº¦']
        values = [
            features['burstiness'],
            features['ttr'],
            features['punctuation_rate'],
            features['top_repeated'],
            features['connector_rate'],
            features['complexity']
        ]
        
        fig = go.Figure(data=go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='ç•¶å‰æ–‡æœ¬',
            line_color='#218D8D'
        ))
        
        fig.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            height=500,
            margin=dict(l=50, r=50, t=50, b=50)
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # è©é »åˆ†ä½ˆ
        if features['word_freq_dist']:
            df_freq = pd.DataFrame([
                {'è©å½™': word, 'é »ç‡': freq}
                for word, freq in features['word_freq_dist'].items()
            ])
            
            fig = px.bar(
                df_freq.head(15),
                x='è©å½™',
                y='é »ç‡',
                title='é »ç‡æœ€é«˜çš„ 15 å€‹è©å½™',
                color='é »ç‡',
                color_continuous_scale='Teal'
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # åˆ†æ•¸å°æ¯”
        fig = go.Figure(data=[
            go.Bar(
                x=['AI ç”Ÿæˆæ©Ÿç‡', 'äººå·¥æ’°å¯«æ©Ÿç‡'],
                y=[result['ai_score'] * 100, result['human_score'] * 100],
                marker_color=['#c01547', '#22c55e'],
                text=[f"{ai_percent}%", f"{human_percent}%"],
                textposition='auto',
                name='åˆ†æ•¸'
            )
        ])
        
        fig.update_layout(
            height=400,
            showlegend=False,
            yaxis_title="ç™¾åˆ†æ¯” (%)",
            yaxis=dict(range=[0, 100])
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # ä¿¡æ¯æç¤º
    st.info("""
    ğŸ’¡ **æç¤ºï¼š**
    - æœ¬å·¥å…·åŸºæ–¼å¤šç¶­åº¦ç‰¹å¾µåˆ†æï¼ˆå¥é•·å·®ç•°ã€è©å½™å¤šæ¨£æ€§ã€ç¬¦è™Ÿä½¿ç”¨ç­‰ï¼‰
    - çµæœç‚ºåƒè€ƒæ€§æŒ‡æ¨™ï¼Œä¸æ§‹æˆæœ€çµ‚è£å®š
    - æœ€ä½³åˆ†ææ–‡æœ¬é•·åº¦ï¼š200+ å­—
    - å°ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„æº–ç¢ºåº¦å¯èƒ½ä¸‹é™
    """)

# æ­·å²è¨˜éŒ„å´é‚Šæ¬„
if st.session_state.get('show_history', False) and st.session_state.analysis_history:
    st.markdown("---")
    st.subheader("ğŸ“‹ åˆ†ææ­·å²")
    
    for i, record in enumerate(reversed(st.session_state.analysis_history[-5:])):
        with st.expander(f"åˆ†æ {i+1} - {record['timestamp'].strftime('%H:%M:%S')}"):
            st.write(f"**é è¦½ï¼š** {record['text_preview']}")
            st.metric("AI æ©Ÿç‡", f"{int(record['ai_score']*100)}%")
            st.metric("Human æ©Ÿç‡", f"{int(record['human_score']*100)}%")
